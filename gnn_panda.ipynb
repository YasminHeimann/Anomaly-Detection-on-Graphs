{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gnn_panda.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Hc51vLuDOjne",
        "ZwXWTlIRKklN",
        "gIKZUNwhMy1W",
        "Y15-dCk9c9Rg",
        "uY3MgwUCc20-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasminHeimann/Graph_Anomaly/blob/master/gnn_panda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc51vLuDOjne"
      },
      "source": [
        "#**Basic PANDA tutorial on graphs, training a 2-layer GCN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUzl_ZC1xGQl",
        "outputId": "1e0a7350-67ed-48a2-9b72-331cf499b089"
      },
      "source": [
        "!pip install dgl\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkDQmLPmhqn1"
      },
      "source": [
        "class Arguments:\n",
        "    dataset = 'cora'\n",
        "    epochs = 100\n",
        "    lr = 0.001\n",
        "    model = 'gcn'\n",
        "    task = 'node_class'\n",
        "    layers = 1\n",
        "    h_dim1 = 16\n",
        "    ewc = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwXWTlIRKklN"
      },
      "source": [
        "#**Get dataset from DGL for a specific task, e.g.,Node Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-3foJATKueH"
      },
      "source": [
        "import dgl.data\n",
        "\n",
        "def get_graph(dataset_name):\n",
        "    if dataset_name =='cora':\n",
        "        dataset = dgl.data.CoraGraphDataset()\n",
        "        # cora db has one graph\n",
        "        g = dataset[0]\n",
        "    elif dataset_name == 'citeseer':\n",
        "        dataset = dgl.data.CiteseerGraphDataset()\n",
        "        # citeseer db has one graph todo ?\n",
        "        g = dataset[0]\n",
        "    else:\n",
        "        print('Default Dataset is cora')\n",
        "        dataset = dgl.data.CoraGraphDataset()\n",
        "        # cora db has one graph\n",
        "        g = dataset[0]\n",
        "    print('\\nNumber of categories:', dataset.num_classes)\n",
        "    print('Node features')\n",
        "    print(g.ndata)\n",
        "    print('Edge features')\n",
        "    print(g.edata)\n",
        "    return g, dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKZUNwhMy1W"
      },
      "source": [
        "#**Define GNN models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsFf7eMoM0oj"
      },
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = dgl.nn.GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "    \n",
        "    def forward(self, g):\n",
        "        in_feat = g.ndata['feat']\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y15-dCk9c9Rg"
      },
      "source": [
        "#**Training Pre-Task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHPdrFCQOEQf"
      },
      "source": [
        "def train(g, model, args):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    labels = g.ndata['label']\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    val_mask = g.ndata['val_mask']\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    for e in range(200):\n",
        "        # Forward\n",
        "        logits = model(g)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
        "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY3MgwUCc20-"
      },
      "source": [
        "#**Get the pre-trained model by arguments given**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKvmxblhc5HI"
      },
      "source": [
        "def get_model_architecture(args, g, dataset):\n",
        "    if args.model == 'gcn':\n",
        "        return GCN(g.ndata['feat'].shape[1], args.h_dim1, dataset.num_classes)  # .to('cuda')\n",
        "    else:\n",
        "        print('Default model architecture is gcn')\n",
        "        return GCN(g.ndata['feat'].shape[1], args.h_dim1, dataset.num_classes)  # .to('cuda')\n",
        "\n",
        "def get_trained_node_class_model(args):\n",
        "    # Create the model with given dimensions\n",
        "    g, dataset = get_graph(args.dataset)\n",
        "    print(\"features dim: \", g.ndata['feat'].shape[1])\n",
        "\n",
        "    model = get_model_architecture(args, g, dataset)\n",
        "    train(g, model, args)\n",
        "\n",
        "    print(\"hello gur\")\n",
        "    print(\"\\nfinished training GCN\\n\")\n",
        "    return model, g, dataset\n",
        "\n",
        "def get_pre_trained_model(args):\n",
        "    if args.task == 'node_class':\n",
        "        return get_trained_node_class_model(args)\n",
        "    else:\n",
        "        print('Default model is node classification with gcn')\n",
        "        return get_node_class_model(args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a93Z4vrYh0Nj"
      },
      "source": [
        "#**PANDA training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZj9oRDCh2jn",
        "outputId": "68af4b18-bde7-4f90-ae82-0c274c910b5d"
      },
      "source": [
        "!apt install libomp-dev\n",
        "!python -m pip install --upgrade faiss\n",
        "# !python -m pip install --upgrade faiss-gpu\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (5.0.1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEg12lw2m0Hg"
      },
      "source": [
        "**knn-faiss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRHCCDRhmDYR"
      },
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "def knn_score(train_set, test_set, n_neighbours=2):\n",
        "    \"\"\"\n",
        "    Calculates the KNN distance\n",
        "    \"\"\"\n",
        "    index = faiss.IndexFlatL2(train_set.shape[1])\n",
        "    index.add(train_set)\n",
        "    D, _ = index.search(test_set, n_neighbours)\n",
        "    return np.sum(D, axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3arGf_R2Nux"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# one big TODO\n",
        "def get_score(model, device, g, num_classes=2, multi_class=False):\n",
        "  train_mask = g.ndata['train_mask']\n",
        "  test_mask = g.ndata['test_mask']\n",
        "  test_labels = g.ndata['label'][test_mask]\n",
        "  labels = (test_labels.cpu().detach().numpy() > 3).astype(int)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      features = model(g)\n",
        "      train_feature_space = features[train_mask]\n",
        "      test_feature_space = features[test_mask]\n",
        "\n",
        "  train_np = train_feature_space.cpu().detach().numpy()\n",
        "  test_np = test_feature_space.cpu().detach().numpy()\n",
        "  # distances from knn\n",
        "  distances = knn_score(train_np, test_np, num_classes)\n",
        "\n",
        "  #auc\n",
        "  # multi class\n",
        "  if multi_class:\n",
        "    one_hot = np.zeros((labels.size, labels.max()+1))\n",
        "    one_hot[np.arange(labels.size),labels] = 1\n",
        "    auc = roc_auc_score(one_hot,distances.reshape(-1, 1), #average='weighted', \n",
        "                        multi_class='ovr')\n",
        "  else:\n",
        "    # todo: is it how to calc auc?\n",
        "    auc = roc_auc_score(labels, distances)\n",
        "\n",
        "  return auc, train_np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCq4KdPq7ntx"
      },
      "source": [
        "**KNN with scikit learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWgGKuAf7rS-"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "def sklearn_knn(test_np, train_np, n_neighbors):\n",
        "  knn = NearestNeighbors(n_neighbors)\n",
        "  knn.fit(train_np)\n",
        "  D, I = knn.kneighbors(test_np)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-6jZD_mkh7M"
      },
      "source": [
        "#**Train Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul8xBpFNjkhR"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class CompactnessLoss(nn.Module):\n",
        "    def __init__(self, center):\n",
        "        super(CompactnessLoss, self).__init__()\n",
        "        self.center = center\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        m = inputs.size(1)\n",
        "        variances = (inputs - self.center).norm(dim=1).pow(2) / m\n",
        "        return variances.mean()\n",
        "\n",
        "\n",
        "def train_panda_model(model, graph, device, args, ewc_loss, num_classes):\n",
        "    model.eval()  # todo\n",
        "    auc, train_feature_space = get_score(model, device, graph, num_classes)\n",
        "    print('Epoch: {}, AUROC is: {}'.format(0, auc))\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=0.00005, momentum=0.9)\n",
        "\n",
        "    #todo how many labels? two?\n",
        "    center = torch.FloatTensor(train_feature_space).mean(dim=0)\n",
        "    criterion = CompactnessLoss(center.to(device))\n",
        "    train_mask = graph.ndata['train_mask']\n",
        "    # todo: is the train mask on two labels? one label? in article?\n",
        "    # todo: i assume only 2 labels in general.\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        running_loss = run_epoch(model, graph, train_mask, optimizer, criterion, device, args.ewc, ewc_loss)\n",
        "        print('Epoch: {}, Loss: {}'.format(epoch + 1, running_loss))\n",
        "        model.eval()\n",
        "        auc, feature_space = get_score(model, device, graph, num_classes)\n",
        "        print('Epoch: {}, AUROC is: {}'.format(epoch + 1, auc))\n",
        "\n",
        "\n",
        "def run_epoch(model, graph, train_mask, optimizer, criterion, device, ewc, ewc_loss):\n",
        "    # todo: images = imgs.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model(graph)\n",
        "\n",
        "    loss = criterion(logits[train_mask])\n",
        "\n",
        "    if ewc:\n",
        "        loss += ewc_loss(model)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-3)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaRfCT8Dh4uT"
      },
      "source": [
        "#**Run pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr0383slQQ1n",
        "outputId": "e170205f-dc9e-437d-b92c-904bfccde377"
      },
      "source": [
        "args = Arguments()\n",
        "model, g, dataset  = get_pre_trained_model(args)\n",
        "print(\"features dim: \", g.ndata['feat'].shape[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "\n",
            "Number of categories: 7\n",
            "Node features\n",
            "{'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False])}\n",
            "Edge features\n",
            "{}\n",
            "features dim:  1433\n",
            "In epoch 0, loss: 1.946, val acc: 0.266 (best 0.266), test acc: 0.267 (best 0.267)\n",
            "In epoch 5, loss: 1.941, val acc: 0.316 (best 0.316), test acc: 0.323 (best 0.323)\n",
            "In epoch 10, loss: 1.936, val acc: 0.432 (best 0.432), test acc: 0.449 (best 0.449)\n",
            "In epoch 15, loss: 1.929, val acc: 0.462 (best 0.462), test acc: 0.442 (best 0.442)\n",
            "In epoch 20, loss: 1.921, val acc: 0.462 (best 0.462), test acc: 0.464 (best 0.442)\n",
            "In epoch 25, loss: 1.913, val acc: 0.492 (best 0.496), test acc: 0.503 (best 0.493)\n",
            "In epoch 30, loss: 1.904, val acc: 0.532 (best 0.532), test acc: 0.546 (best 0.546)\n",
            "In epoch 35, loss: 1.894, val acc: 0.578 (best 0.578), test acc: 0.597 (best 0.597)\n",
            "In epoch 40, loss: 1.885, val acc: 0.586 (best 0.588), test acc: 0.615 (best 0.610)\n",
            "In epoch 45, loss: 1.875, val acc: 0.614 (best 0.614), test acc: 0.628 (best 0.628)\n",
            "In epoch 50, loss: 1.864, val acc: 0.618 (best 0.618), test acc: 0.645 (best 0.642)\n",
            "In epoch 55, loss: 1.854, val acc: 0.618 (best 0.620), test acc: 0.653 (best 0.646)\n",
            "In epoch 60, loss: 1.842, val acc: 0.622 (best 0.622), test acc: 0.657 (best 0.657)\n",
            "In epoch 65, loss: 1.831, val acc: 0.636 (best 0.636), test acc: 0.666 (best 0.666)\n",
            "In epoch 70, loss: 1.819, val acc: 0.642 (best 0.642), test acc: 0.678 (best 0.677)\n",
            "In epoch 75, loss: 1.806, val acc: 0.650 (best 0.650), test acc: 0.686 (best 0.682)\n",
            "In epoch 80, loss: 1.793, val acc: 0.658 (best 0.658), test acc: 0.685 (best 0.685)\n",
            "In epoch 85, loss: 1.780, val acc: 0.664 (best 0.664), test acc: 0.690 (best 0.688)\n",
            "In epoch 90, loss: 1.766, val acc: 0.670 (best 0.670), test acc: 0.698 (best 0.693)\n",
            "In epoch 95, loss: 1.752, val acc: 0.672 (best 0.672), test acc: 0.705 (best 0.705)\n",
            "In epoch 100, loss: 1.738, val acc: 0.678 (best 0.678), test acc: 0.706 (best 0.707)\n",
            "In epoch 105, loss: 1.723, val acc: 0.680 (best 0.680), test acc: 0.704 (best 0.706)\n",
            "In epoch 110, loss: 1.708, val acc: 0.686 (best 0.686), test acc: 0.704 (best 0.704)\n",
            "In epoch 115, loss: 1.692, val acc: 0.688 (best 0.688), test acc: 0.713 (best 0.709)\n",
            "In epoch 120, loss: 1.676, val acc: 0.692 (best 0.692), test acc: 0.714 (best 0.713)\n",
            "In epoch 125, loss: 1.659, val acc: 0.690 (best 0.692), test acc: 0.711 (best 0.713)\n",
            "In epoch 130, loss: 1.643, val acc: 0.694 (best 0.694), test acc: 0.714 (best 0.712)\n",
            "In epoch 135, loss: 1.626, val acc: 0.700 (best 0.700), test acc: 0.713 (best 0.713)\n",
            "In epoch 140, loss: 1.608, val acc: 0.696 (best 0.702), test acc: 0.713 (best 0.712)\n",
            "In epoch 145, loss: 1.591, val acc: 0.696 (best 0.702), test acc: 0.716 (best 0.712)\n",
            "In epoch 150, loss: 1.573, val acc: 0.702 (best 0.702), test acc: 0.723 (best 0.712)\n",
            "In epoch 155, loss: 1.555, val acc: 0.704 (best 0.704), test acc: 0.727 (best 0.727)\n",
            "In epoch 160, loss: 1.536, val acc: 0.706 (best 0.706), test acc: 0.725 (best 0.725)\n",
            "In epoch 165, loss: 1.517, val acc: 0.704 (best 0.708), test acc: 0.726 (best 0.725)\n",
            "In epoch 170, loss: 1.498, val acc: 0.706 (best 0.708), test acc: 0.729 (best 0.725)\n",
            "In epoch 175, loss: 1.479, val acc: 0.708 (best 0.708), test acc: 0.729 (best 0.725)\n",
            "In epoch 180, loss: 1.460, val acc: 0.710 (best 0.710), test acc: 0.728 (best 0.730)\n",
            "In epoch 185, loss: 1.441, val acc: 0.712 (best 0.714), test acc: 0.728 (best 0.728)\n",
            "In epoch 190, loss: 1.421, val acc: 0.712 (best 0.714), test acc: 0.727 (best 0.728)\n",
            "In epoch 195, loss: 1.401, val acc: 0.712 (best 0.714), test acc: 0.724 (best 0.728)\n",
            "hello gur\n",
            "\n",
            "finished training GCN\n",
            "\n",
            "features dim:  1433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwiF8J20mD__",
        "outputId": "9586b2b5-a3d6-4d1a-aa74-8090e78f7669"
      },
      "source": [
        "# ewc None\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# test with 2 labels only - take half > 3 as 0 label and the rest 1 (1800, 900 samples)\n",
        "\n",
        "# todo: better architecture, GAT https://paperswithcode.com/paper/graph-attention-networks\n",
        "# todo: take only 2 labels (e.g., 0 and 1 or 0 and 3)\n",
        "# todo: is train only on label?\n",
        "\n",
        "# for 3: 800 samples, others ~200-300\n",
        "train_panda_model(model, g, device, args, None, 2)\n",
        "\n",
        "\n",
        "# self training:\n",
        "# g, dataset = get_graph(args.dataset)\n",
        "# #model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
        "# model = get_model(args)\n",
        "# train(g, model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, AUROC is: 0.519149270856466\n",
            "Epoch: 1, Loss: 0.11761083453893661\n",
            "Epoch: 1, AUROC is: 0.519149270856466\n",
            "Epoch: 2, Loss: 0.11761071532964706\n",
            "Epoch: 2, AUROC is: 0.519149270856466\n",
            "Epoch: 3, Loss: 0.11761046946048737\n",
            "Epoch: 3, AUROC is: 0.5191446443112\n",
            "Epoch: 4, Loss: 0.11761009693145752\n",
            "Epoch: 4, AUROC is: 0.5191400177659338\n",
            "Epoch: 5, Loss: 0.11760963499546051\n",
            "Epoch: 5, AUROC is: 0.5191400177659338\n",
            "Epoch: 6, Loss: 0.11760909110307693\n",
            "Epoch: 6, AUROC is: 0.519149270856466\n",
            "Epoch: 7, Loss: 0.1176084652543068\n",
            "Epoch: 7, AUROC is: 0.5191538974017321\n",
            "Epoch: 8, Loss: 0.11760775744915009\n",
            "Epoch: 8, AUROC is: 0.5191631504922644\n",
            "Epoch: 9, Loss: 0.117606982588768\n",
            "Epoch: 9, AUROC is: 0.5191585239469982\n",
            "Epoch: 10, Loss: 0.11760617792606354\n",
            "Epoch: 10, AUROC is: 0.5191608372196314\n",
            "Epoch: 11, Loss: 0.1176052913069725\n",
            "Epoch: 11, AUROC is: 0.5191677770375305\n",
            "Epoch: 12, Loss: 0.11760436743497849\n",
            "Epoch: 12, AUROC is: 0.5191724035827967\n",
            "Epoch: 13, Loss: 0.11760339885950089\n",
            "Epoch: 13, AUROC is: 0.5191724035827967\n",
            "Epoch: 14, Loss: 0.1176024004817009\n",
            "Epoch: 14, AUROC is: 0.5191770301280627\n",
            "Epoch: 15, Loss: 0.11760136485099792\n",
            "Epoch: 15, AUROC is: 0.5191770301280627\n",
            "Epoch: 16, Loss: 0.11760029196739197\n",
            "Epoch: 16, AUROC is: 0.5191770301280627\n",
            "Epoch: 17, Loss: 0.11759918928146362\n",
            "Epoch: 17, AUROC is: 0.5191770301280627\n",
            "Epoch: 18, Loss: 0.11759805679321289\n",
            "Epoch: 18, AUROC is: 0.5191677770375305\n",
            "Epoch: 19, Loss: 0.11759690195322037\n",
            "Epoch: 19, AUROC is: 0.5191770301280627\n",
            "Epoch: 20, Loss: 0.11759573966264725\n",
            "Epoch: 20, AUROC is: 0.5191816566733289\n",
            "Epoch: 21, Loss: 0.11759455502033234\n",
            "Epoch: 21, AUROC is: 0.519186283218595\n",
            "Epoch: 22, Loss: 0.11759335547685623\n",
            "Epoch: 22, AUROC is: 0.5191816566733289\n",
            "Epoch: 23, Loss: 0.11759214103221893\n",
            "Epoch: 23, AUROC is: 0.5191770301280627\n",
            "Epoch: 24, Loss: 0.11759090423583984\n",
            "Epoch: 24, AUROC is: 0.5191816566733289\n",
            "Epoch: 25, Loss: 0.11758965253829956\n",
            "Epoch: 25, AUROC is: 0.5191816566733289\n",
            "Epoch: 26, Loss: 0.11758840084075928\n",
            "Epoch: 26, AUROC is: 0.5191839699459619\n",
            "Epoch: 27, Loss: 0.1175871416926384\n",
            "Epoch: 27, AUROC is: 0.519186283218595\n",
            "Epoch: 28, Loss: 0.11758585274219513\n",
            "Epoch: 28, AUROC is: 0.5191816566733289\n",
            "Epoch: 29, Loss: 0.11758457124233246\n",
            "Epoch: 29, AUROC is: 0.5191770301280627\n",
            "Epoch: 30, Loss: 0.1175832748413086\n",
            "Epoch: 30, AUROC is: 0.5191770301280627\n",
            "Epoch: 31, Loss: 0.11758195608854294\n",
            "Epoch: 31, AUROC is: 0.5191724035827967\n",
            "Epoch: 32, Loss: 0.11758065968751907\n",
            "Epoch: 32, AUROC is: 0.5191770301280627\n",
            "Epoch: 33, Loss: 0.11757934093475342\n",
            "Epoch: 33, AUROC is: 0.5191770301280627\n",
            "Epoch: 34, Loss: 0.11757801473140717\n",
            "Epoch: 34, AUROC is: 0.5191793434006958\n",
            "Epoch: 35, Loss: 0.11757669597864151\n",
            "Epoch: 35, AUROC is: 0.5191816566733289\n",
            "Epoch: 36, Loss: 0.11757536232471466\n",
            "Epoch: 36, AUROC is: 0.5191909097638612\n",
            "Epoch: 37, Loss: 0.11757403612136841\n",
            "Epoch: 37, AUROC is: 0.519186283218595\n",
            "Epoch: 38, Loss: 0.11757270246744156\n",
            "Epoch: 38, AUROC is: 0.5191816566733289\n",
            "Epoch: 39, Loss: 0.1175713837146759\n",
            "Epoch: 39, AUROC is: 0.5191816566733289\n",
            "Epoch: 40, Loss: 0.11757003515958786\n",
            "Epoch: 40, AUROC is: 0.5191816566733289\n",
            "Epoch: 41, Loss: 0.11756869405508041\n",
            "Epoch: 41, AUROC is: 0.5191909097638612\n",
            "Epoch: 42, Loss: 0.11756734549999237\n",
            "Epoch: 42, AUROC is: 0.519186283218595\n",
            "Epoch: 43, Loss: 0.11756599694490433\n",
            "Epoch: 43, AUROC is: 0.519186283218595\n",
            "Epoch: 44, Loss: 0.11756464838981628\n",
            "Epoch: 44, AUROC is: 0.5191816566733289\n",
            "Epoch: 45, Loss: 0.11756329983472824\n",
            "Epoch: 45, AUROC is: 0.5191770301280627\n",
            "Epoch: 46, Loss: 0.1175619512796402\n",
            "Epoch: 46, AUROC is: 0.5191677770375305\n",
            "Epoch: 47, Loss: 0.11756058782339096\n",
            "Epoch: 47, AUROC is: 0.5191677770375305\n",
            "Epoch: 48, Loss: 0.11755921691656113\n",
            "Epoch: 48, AUROC is: 0.519149270856466\n",
            "Epoch: 49, Loss: 0.11755786836147308\n",
            "Epoch: 49, AUROC is: 0.5191538974017321\n",
            "Epoch: 50, Loss: 0.11755650490522385\n",
            "Epoch: 50, AUROC is: 0.5191538974017321\n",
            "Epoch: 51, Loss: 0.11755512654781342\n",
            "Epoch: 51, AUROC is: 0.5191585239469984\n",
            "Epoch: 52, Loss: 0.11755376309156418\n",
            "Epoch: 52, AUROC is: 0.5191631504922645\n",
            "Epoch: 53, Loss: 0.11755239218473434\n",
            "Epoch: 53, AUROC is: 0.5191631504922645\n",
            "Epoch: 54, Loss: 0.11755101382732391\n",
            "Epoch: 54, AUROC is: 0.5191631504922645\n",
            "Epoch: 55, Loss: 0.11754963546991348\n",
            "Epoch: 55, AUROC is: 0.5191631504922644\n",
            "Epoch: 56, Loss: 0.11754826456308365\n",
            "Epoch: 56, AUROC is: 0.5191585239469982\n",
            "Epoch: 57, Loss: 0.11754690110683441\n",
            "Epoch: 57, AUROC is: 0.5191631504922644\n",
            "Epoch: 58, Loss: 0.11754550784826279\n",
            "Epoch: 58, AUROC is: 0.5191585239469984\n",
            "Epoch: 59, Loss: 0.11754413694143295\n",
            "Epoch: 59, AUROC is: 0.5191631504922645\n",
            "Epoch: 60, Loss: 0.11754275858402252\n",
            "Epoch: 60, AUROC is: 0.5191631504922645\n",
            "Epoch: 61, Loss: 0.11754138022661209\n",
            "Epoch: 61, AUROC is: 0.5191677770375305\n",
            "Epoch: 62, Loss: 0.11753999441862106\n",
            "Epoch: 62, AUROC is: 0.5191677770375305\n",
            "Epoch: 63, Loss: 0.11753861606121063\n",
            "Epoch: 63, AUROC is: 0.5191631504922645\n",
            "Epoch: 64, Loss: 0.1175372377038002\n",
            "Epoch: 64, AUROC is: 0.5191585239469982\n",
            "Epoch: 65, Loss: 0.11753587424755096\n",
            "Epoch: 65, AUROC is: 0.5191585239469982\n",
            "Epoch: 66, Loss: 0.11753450334072113\n",
            "Epoch: 66, AUROC is: 0.5191631504922645\n",
            "Epoch: 67, Loss: 0.1175331100821495\n",
            "Epoch: 67, AUROC is: 0.5191631504922645\n",
            "Epoch: 68, Loss: 0.11753173917531967\n",
            "Epoch: 68, AUROC is: 0.5191631504922645\n",
            "Epoch: 69, Loss: 0.11753036081790924\n",
            "Epoch: 69, AUROC is: 0.5191677770375306\n",
            "Epoch: 70, Loss: 0.11752899736166\n",
            "Epoch: 70, AUROC is: 0.5191677770375306\n",
            "Epoch: 71, Loss: 0.11752760410308838\n",
            "Epoch: 71, AUROC is: 0.5191677770375305\n",
            "Epoch: 72, Loss: 0.11752624809741974\n",
            "Epoch: 72, AUROC is: 0.5191724035827967\n",
            "Epoch: 73, Loss: 0.11752485483884811\n",
            "Epoch: 73, AUROC is: 0.5191724035827967\n",
            "Epoch: 74, Loss: 0.11752347648143768\n",
            "Epoch: 74, AUROC is: 0.5191724035827967\n",
            "Epoch: 75, Loss: 0.11752210557460785\n",
            "Epoch: 75, AUROC is: 0.5191770301280627\n",
            "Epoch: 76, Loss: 0.11752071231603622\n",
            "Epoch: 76, AUROC is: 0.5191770301280627\n",
            "Epoch: 77, Loss: 0.11751934140920639\n",
            "Epoch: 77, AUROC is: 0.5191816566733288\n",
            "Epoch: 78, Loss: 0.11751797795295715\n",
            "Epoch: 78, AUROC is: 0.5191770301280628\n",
            "Epoch: 79, Loss: 0.11751656979322433\n",
            "Epoch: 79, AUROC is: 0.5191770301280628\n",
            "Epoch: 80, Loss: 0.1175151988863945\n",
            "Epoch: 80, AUROC is: 0.5191770301280628\n",
            "Epoch: 81, Loss: 0.11751382052898407\n",
            "Epoch: 81, AUROC is: 0.5191770301280628\n",
            "Epoch: 82, Loss: 0.11751242727041245\n",
            "Epoch: 82, AUROC is: 0.5191816566733289\n",
            "Epoch: 83, Loss: 0.11751105636358261\n",
            "Epoch: 83, AUROC is: 0.5191816566733289\n",
            "Epoch: 84, Loss: 0.11750967800617218\n",
            "Epoch: 84, AUROC is: 0.5191724035827967\n",
            "Epoch: 85, Loss: 0.11750829219818115\n",
            "Epoch: 85, AUROC is: 0.5191770301280627\n",
            "Epoch: 86, Loss: 0.11750689893960953\n",
            "Epoch: 86, AUROC is: 0.5191770301280627\n",
            "Epoch: 87, Loss: 0.1175055131316185\n",
            "Epoch: 87, AUROC is: 0.5191770301280627\n",
            "Epoch: 88, Loss: 0.11750411987304688\n",
            "Epoch: 88, AUROC is: 0.5191770301280628\n",
            "Epoch: 89, Loss: 0.11750274151563644\n",
            "Epoch: 89, AUROC is: 0.5191770301280628\n",
            "Epoch: 90, Loss: 0.11750134080648422\n",
            "Epoch: 90, AUROC is: 0.5191770301280628\n",
            "Epoch: 91, Loss: 0.1174999475479126\n",
            "Epoch: 91, AUROC is: 0.5191862832185951\n",
            "Epoch: 92, Loss: 0.11749856173992157\n",
            "Epoch: 92, AUROC is: 0.5191885964912281\n",
            "Epoch: 93, Loss: 0.11749716848134995\n",
            "Epoch: 93, AUROC is: 0.5191793434006959\n",
            "Epoch: 94, Loss: 0.11749578267335892\n",
            "Epoch: 94, AUROC is: 0.5191816566733289\n",
            "Epoch: 95, Loss: 0.11749438941478729\n",
            "Epoch: 95, AUROC is: 0.5191816566733289\n",
            "Epoch: 96, Loss: 0.11749300360679626\n",
            "Epoch: 96, AUROC is: 0.5191770301280627\n",
            "Epoch: 97, Loss: 0.11749161034822464\n",
            "Epoch: 97, AUROC is: 0.5191724035827967\n",
            "Epoch: 98, Loss: 0.11749022454023361\n",
            "Epoch: 98, AUROC is: 0.5191770301280627\n",
            "Epoch: 99, Loss: 0.11748883128166199\n",
            "Epoch: 99, AUROC is: 0.5191770301280627\n",
            "Epoch: 100, Loss: 0.11748744547367096\n",
            "Epoch: 100, AUROC is: 0.5191816566733289\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}