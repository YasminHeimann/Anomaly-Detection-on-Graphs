C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\venv_ssl\Scripts\python.exe C:/Users/Yasmin/PycharmProjects/Graph_Anomaly/panda.py --datapath C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SelfTask-GNN\data --lambda 1 --dataset cora --ssl EdgeMask --epochs 100
Using backend: pytorch
Analysing ssl tasks
running on:  cora EdgeMask
Load full supervised task.
nclass: 7	nfeat:1433
=== Number of Total Layers is 3 ===
Test set results: loss= 0.4457 auc= 0.9783 accuracy= 0.8660
accuracy=0.86600
0.246031	0.000000	0.445733	0.924669	0.000000	0.866000
Self-Supervised Type: EdgeMask
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='cora', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=10, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=0, sampling_percent=1.0, seed=42, ssl='EdgeMask', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.4460856720827179
saving model: EdgeMask on dataset:  cora
running on:  cora PairwiseDistance
Load full supervised task.
nclass: 7	nfeat:1433
=== Number of Total Layers is 3 ===
tensor(1.3874, grad_fn=<NllLossBackward>)
tensor(1.3825, grad_fn=<NllLossBackward>)
tensor(1.3746, grad_fn=<NllLossBackward>)
tensor(1.3643, grad_fn=<NllLossBackward>)
tensor(1.3500, grad_fn=<NllLossBackward>)
tensor(1.3358, grad_fn=<NllLossBackward>)
tensor(1.3129, grad_fn=<NllLossBackward>)
tensor(1.2981, grad_fn=<NllLossBackward>)
tensor(1.2821, grad_fn=<NllLossBackward>)
tensor(1.2709, grad_fn=<NllLossBackward>)
tensor(1.2587, grad_fn=<NllLossBackward>)
tensor(1.2447, grad_fn=<NllLossBackward>)
tensor(1.2331, grad_fn=<NllLossBackward>)
tensor(1.2288, grad_fn=<NllLossBackward>)
tensor(1.2204, grad_fn=<NllLossBackward>)
tensor(1.2156, grad_fn=<NllLossBackward>)
tensor(1.2134, grad_fn=<NllLossBackward>)
tensor(1.1988, grad_fn=<NllLossBackward>)
tensor(1.1908, grad_fn=<NllLossBackward>)
tensor(1.1863, grad_fn=<NllLossBackward>)
tensor(1.1761, grad_fn=<NllLossBackward>)
tensor(1.1688, grad_fn=<NllLossBackward>)
tensor(1.1783, grad_fn=<NllLossBackward>)
tensor(1.1684, grad_fn=<NllLossBackward>)
tensor(1.1585, grad_fn=<NllLossBackward>)
tensor(1.1490, grad_fn=<NllLossBackward>)
tensor(1.1548, grad_fn=<NllLossBackward>)
tensor(1.1433, grad_fn=<NllLossBackward>)
tensor(1.1381, grad_fn=<NllLossBackward>)
tensor(1.1412, grad_fn=<NllLossBackward>)
tensor(1.1248, grad_fn=<NllLossBackward>)
tensor(1.1305, grad_fn=<NllLossBackward>)
tensor(1.1180, grad_fn=<NllLossBackward>)
tensor(1.1216, grad_fn=<NllLossBackward>)
tensor(1.1220, grad_fn=<NllLossBackward>)
tensor(1.1174, grad_fn=<NllLossBackward>)
tensor(1.1068, grad_fn=<NllLossBackward>)
tensor(1.1076, grad_fn=<NllLossBackward>)
tensor(1.1041, grad_fn=<NllLossBackward>)
tensor(1.1096, grad_fn=<NllLossBackward>)
tensor(1.0970, grad_fn=<NllLossBackward>)
tensor(1.1040, grad_fn=<NllLossBackward>)
tensor(1.0958, grad_fn=<NllLossBackward>)
tensor(1.0970, grad_fn=<NllLossBackward>)
tensor(1.0882, grad_fn=<NllLossBackward>)
tensor(1.0837, grad_fn=<NllLossBackward>)
tensor(1.0820, grad_fn=<NllLossBackward>)
tensor(1.0926, grad_fn=<NllLossBackward>)
tensor(1.0782, grad_fn=<NllLossBackward>)
tensor(1.0798, grad_fn=<NllLossBackward>)
tensor(1.0738, grad_fn=<NllLossBackward>)
tensor(1.0633, grad_fn=<NllLossBackward>)
tensor(1.0660, grad_fn=<NllLossBackward>)
tensor(1.0667, grad_fn=<NllLossBackward>)
tensor(1.0682, grad_fn=<NllLossBackward>)
tensor(1.0634, grad_fn=<NllLossBackward>)
tensor(1.0584, grad_fn=<NllLossBackward>)
tensor(1.0684, grad_fn=<NllLossBackward>)
tensor(1.0538, grad_fn=<NllLossBackward>)
tensor(1.0563, grad_fn=<NllLossBackward>)
tensor(1.0558, grad_fn=<NllLossBackward>)
tensor(1.0646, grad_fn=<NllLossBackward>)
tensor(1.0540, grad_fn=<NllLossBackward>)
tensor(1.0519, grad_fn=<NllLossBackward>)
tensor(1.0531, grad_fn=<NllLossBackward>)
tensor(1.0505, grad_fn=<NllLossBackward>)
tensor(1.0490, grad_fn=<NllLossBackward>)
tensor(1.0393, grad_fn=<NllLossBackward>)
tensor(1.0496, grad_fn=<NllLossBackward>)
tensor(1.0354, grad_fn=<NllLossBackward>)
tensor(1.0419, grad_fn=<NllLossBackward>)
tensor(1.0338, grad_fn=<NllLossBackward>)
tensor(1.0469, grad_fn=<NllLossBackward>)
tensor(1.0386, grad_fn=<NllLossBackward>)
tensor(1.0387, grad_fn=<NllLossBackward>)
tensor(1.0389, grad_fn=<NllLossBackward>)
tensor(1.0307, grad_fn=<NllLossBackward>)
tensor(1.0275, grad_fn=<NllLossBackward>)
tensor(1.0255, grad_fn=<NllLossBackward>)
tensor(1.0315, grad_fn=<NllLossBackward>)
tensor(1.0275, grad_fn=<NllLossBackward>)
tensor(1.0158, grad_fn=<NllLossBackward>)
tensor(1.0277, grad_fn=<NllLossBackward>)
tensor(1.0230, grad_fn=<NllLossBackward>)
tensor(1.0337, grad_fn=<NllLossBackward>)
tensor(1.0216, grad_fn=<NllLossBackward>)
tensor(1.0207, grad_fn=<NllLossBackward>)
tensor(1.0119, grad_fn=<NllLossBackward>)
tensor(1.0149, grad_fn=<NllLossBackward>)
tensor(1.0245, grad_fn=<NllLossBackward>)
tensor(1.0091, grad_fn=<NllLossBackward>)
tensor(1.0163, grad_fn=<NllLossBackward>)
tensor(1.0090, grad_fn=<NllLossBackward>)
tensor(1.0062, grad_fn=<NllLossBackward>)
tensor(1.0131, grad_fn=<NllLossBackward>)
tensor(1.0123, grad_fn=<NllLossBackward>)
tensor(1.0154, grad_fn=<NllLossBackward>)
tensor(1.0153, grad_fn=<NllLossBackward>)
tensor(1.0081, grad_fn=<NllLossBackward>)
tensor(1.0128, grad_fn=<NllLossBackward>)
Test set results: loss= 0.4076 auc= 0.9819 accuracy= 0.8790
accuracy=0.87900
0.252435	0.000000	0.407626	0.923841	0.000000	0.879000
Self-Supervised Type: PairwiseDistance
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='cora', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=20, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=0, sampling_percent=1.0, seed=42, ssl='PairwiseDistance', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.4460856720827179
saving model: PairwiseDistance on dataset:  cora
running on:  cora PairwiseAttrSim
Load full supervised task.
nclass: 7	nfeat:1433
=== Number of Total Layers is 3 ===
loading cora_3_attrsim_sampled_idx.npy
number of sampled: 22617
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\selfsl.py:632: UserWarning: Using a target size (torch.Size([5000])) that is different to the input size (torch.Size([5000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss = F.mse_loss(embeddings, self.pseudo_labels[sampled], reduction='mean')
Test set results: loss= 0.4637 auc= 0.9770 accuracy= 0.8640
accuracy=0.86400
0.182703	0.000000	0.463693	0.947020	0.000000	0.864000
Self-Supervised Type: PairwiseAttrSim
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='cora', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=100, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=0, sampling_percent=1.0, seed=42, ssl='PairwiseAttrSim', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.4460856720827179
saving model: PairwiseAttrSim on dataset:  cora
running on:  cora Distance2Labeled
Load full supervised task.
nclass: 7	nfeat:1433
=== Number of Total Layers is 3 ===
Test set results: loss= 0.4587 auc= 0.9778 accuracy= 0.8600
accuracy=0.86000
0.172855	0.000000	0.458734	0.947020	0.000000	0.860000
Self-Supervised Type: Distance2Labeled
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='cora', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=100, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=0, sampling_percent=1.0, seed=42, ssl='Distance2Labeled', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.4460856720827179
saving model: Distance2Labeled on dataset:  cora
running on:  cora AttributeMask
Load full supervised task.
nclass: 7	nfeat:256
=== Number of Total Layers is 3 ===
Test set results: loss= 0.5168 auc= 0.9724 accuracy= 0.8430
accuracy=0.84300
0.214716	0.000000	0.516802	0.933775	0.000000	0.843000
Self-Supervised Type: AttributeMask~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='cora', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=100, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='AttributeMask~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.4460856720827179
saving model: AttributeMask~PCA on dataset:  cora
running on:  cora NodeProperty
Load full supervised task.
nclass: 7	nfeat:256
=== Number of Total Layers is 3 ===
Test set results: loss= 0.5141 auc= 0.9726 accuracy= 0.8430
accuracy=0.84300
0.248059	0.000000	0.514135	0.924669	0.000000	0.843000
Self-Supervised Type: NodeProperty~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='cora', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=5, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='NodeProperty~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.4460856720827179
saving model: NodeProperty~PCA on dataset:  cora
running on:  citeseer EdgeMask
Load full supervised task.
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\normalization.py:116: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
nclass: 6	nfeat:256
=== Number of Total Layers is 3 ===
Test set results: loss= 0.7691 auc= 0.9239 accuracy= 0.7700
accuracy=0.77000
0.541626	0.000000	0.769058	0.814018	0.000000	0.770000
Self-Supervised Type: EdgeMask~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='citeseer', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=5, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='EdgeMask~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.5446348061316502
saving model: EdgeMask~PCA on dataset:  citeseer
running on:  citeseer PairwiseDistance
Load full supervised task.
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\normalization.py:116: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
nclass: 6	nfeat:256
=== Number of Total Layers is 3 ===
tensor(1.3883, grad_fn=<NllLossBackward>)
tensor(1.3855, grad_fn=<NllLossBackward>)
tensor(1.3853, grad_fn=<NllLossBackward>)
tensor(1.3830, grad_fn=<NllLossBackward>)
tensor(1.3810, grad_fn=<NllLossBackward>)
tensor(1.3783, grad_fn=<NllLossBackward>)
tensor(1.3754, grad_fn=<NllLossBackward>)
tensor(1.3708, grad_fn=<NllLossBackward>)
tensor(1.3665, grad_fn=<NllLossBackward>)
tensor(1.3601, grad_fn=<NllLossBackward>)
tensor(1.3548, grad_fn=<NllLossBackward>)
tensor(1.3478, grad_fn=<NllLossBackward>)
tensor(1.3405, grad_fn=<NllLossBackward>)
tensor(1.3346, grad_fn=<NllLossBackward>)
tensor(1.3259, grad_fn=<NllLossBackward>)
tensor(1.3165, grad_fn=<NllLossBackward>)
tensor(1.3116, grad_fn=<NllLossBackward>)
tensor(1.3058, grad_fn=<NllLossBackward>)
tensor(1.2987, grad_fn=<NllLossBackward>)
tensor(1.2914, grad_fn=<NllLossBackward>)
tensor(1.2930, grad_fn=<NllLossBackward>)
tensor(1.2835, grad_fn=<NllLossBackward>)
tensor(1.2774, grad_fn=<NllLossBackward>)
tensor(1.2683, grad_fn=<NllLossBackward>)
tensor(1.2630, grad_fn=<NllLossBackward>)
tensor(1.2624, grad_fn=<NllLossBackward>)
tensor(1.2536, grad_fn=<NllLossBackward>)
tensor(1.2572, grad_fn=<NllLossBackward>)
tensor(1.2420, grad_fn=<NllLossBackward>)
tensor(1.2415, grad_fn=<NllLossBackward>)
tensor(1.2410, grad_fn=<NllLossBackward>)
tensor(1.2350, grad_fn=<NllLossBackward>)
tensor(1.2300, grad_fn=<NllLossBackward>)
tensor(1.2309, grad_fn=<NllLossBackward>)
tensor(1.2290, grad_fn=<NllLossBackward>)
tensor(1.2150, grad_fn=<NllLossBackward>)
tensor(1.2156, grad_fn=<NllLossBackward>)
tensor(1.2144, grad_fn=<NllLossBackward>)
tensor(1.2145, grad_fn=<NllLossBackward>)
tensor(1.2140, grad_fn=<NllLossBackward>)
tensor(1.2069, grad_fn=<NllLossBackward>)
tensor(1.2059, grad_fn=<NllLossBackward>)
tensor(1.2025, grad_fn=<NllLossBackward>)
tensor(1.1993, grad_fn=<NllLossBackward>)
tensor(1.1995, grad_fn=<NllLossBackward>)
tensor(1.1907, grad_fn=<NllLossBackward>)
tensor(1.1918, grad_fn=<NllLossBackward>)
tensor(1.1892, grad_fn=<NllLossBackward>)
tensor(1.1898, grad_fn=<NllLossBackward>)
tensor(1.1834, grad_fn=<NllLossBackward>)
tensor(1.1830, grad_fn=<NllLossBackward>)
tensor(1.1849, grad_fn=<NllLossBackward>)
tensor(1.1814, grad_fn=<NllLossBackward>)
tensor(1.1796, grad_fn=<NllLossBackward>)
tensor(1.1833, grad_fn=<NllLossBackward>)
tensor(1.1727, grad_fn=<NllLossBackward>)
tensor(1.1734, grad_fn=<NllLossBackward>)
tensor(1.1782, grad_fn=<NllLossBackward>)
tensor(1.1693, grad_fn=<NllLossBackward>)
tensor(1.1699, grad_fn=<NllLossBackward>)
tensor(1.1671, grad_fn=<NllLossBackward>)
tensor(1.1679, grad_fn=<NllLossBackward>)
tensor(1.1713, grad_fn=<NllLossBackward>)
tensor(1.1638, grad_fn=<NllLossBackward>)
tensor(1.1670, grad_fn=<NllLossBackward>)
tensor(1.1560, grad_fn=<NllLossBackward>)
tensor(1.1574, grad_fn=<NllLossBackward>)
tensor(1.1540, grad_fn=<NllLossBackward>)
tensor(1.1573, grad_fn=<NllLossBackward>)
tensor(1.1453, grad_fn=<NllLossBackward>)
tensor(1.1529, grad_fn=<NllLossBackward>)
tensor(1.1471, grad_fn=<NllLossBackward>)
tensor(1.1496, grad_fn=<NllLossBackward>)
tensor(1.1525, grad_fn=<NllLossBackward>)
tensor(1.1518, grad_fn=<NllLossBackward>)
tensor(1.1474, grad_fn=<NllLossBackward>)
tensor(1.1450, grad_fn=<NllLossBackward>)
tensor(1.1420, grad_fn=<NllLossBackward>)
tensor(1.1396, grad_fn=<NllLossBackward>)
tensor(1.1416, grad_fn=<NllLossBackward>)
tensor(1.1336, grad_fn=<NllLossBackward>)
tensor(1.1339, grad_fn=<NllLossBackward>)
tensor(1.1405, grad_fn=<NllLossBackward>)
tensor(1.1346, grad_fn=<NllLossBackward>)
tensor(1.1405, grad_fn=<NllLossBackward>)
tensor(1.1422, grad_fn=<NllLossBackward>)
tensor(1.1362, grad_fn=<NllLossBackward>)
tensor(1.1378, grad_fn=<NllLossBackward>)
tensor(1.1263, grad_fn=<NllLossBackward>)
tensor(1.1252, grad_fn=<NllLossBackward>)
tensor(1.1271, grad_fn=<NllLossBackward>)
tensor(1.1360, grad_fn=<NllLossBackward>)
tensor(1.1223, grad_fn=<NllLossBackward>)
tensor(1.1245, grad_fn=<NllLossBackward>)
tensor(1.1290, grad_fn=<NllLossBackward>)
tensor(1.1298, grad_fn=<NllLossBackward>)
tensor(1.1277, grad_fn=<NllLossBackward>)
tensor(1.1235, grad_fn=<NllLossBackward>)
tensor(1.1177, grad_fn=<NllLossBackward>)
tensor(1.1216, grad_fn=<NllLossBackward>)
Test set results: loss= 0.7429 auc= 0.9290 accuracy= 0.7790
accuracy=0.77900
0.485012	0.000000	0.742903	0.836093	0.000000	0.779000
Self-Supervised Type: PairwiseDistance~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='citeseer', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=5, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='PairwiseDistance~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.5446348061316502
saving model: PairwiseDistance~PCA on dataset:  citeseer
running on:  citeseer PairwiseAttrSim
Load full supervised task.
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\normalization.py:116: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
nclass: 6	nfeat:256
=== Number of Total Layers is 3 ===
loading citeseer_3_attrsim_sampled_idx.npy
number of sampled: 26380
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\selfsl.py:632: UserWarning: Using a target size (torch.Size([5000])) that is different to the input size (torch.Size([5000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss = F.mse_loss(embeddings, self.pseudo_labels[sampled], reduction='mean')
Test set results: loss= 0.7856 auc= 0.9238 accuracy= 0.7630
accuracy=0.76300
0.410811	0.000000	0.785612	0.855960	0.000000	0.763000
Self-Supervised Type: PairwiseAttrSim~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='citeseer', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=5, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='PairwiseAttrSim~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.5446348061316502
saving model: PairwiseAttrSim~PCA on dataset:  citeseer
running on:  citeseer Distance2Labeled
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\normalization.py:116: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
Load full supervised task.
nclass: 6	nfeat:256
=== Number of Total Layers is 3 ===
Test set results: loss= 0.7798 auc= 0.9238 accuracy= 0.7680
accuracy=0.76800
0.399736	0.000000	0.779841	0.854857	0.000000	0.768000
Self-Supervised Type: Distance2Labeled~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='citeseer', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=10, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='Distance2Labeled~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.5446348061316502
saving model: Distance2Labeled~PCA on dataset:  citeseer
running on:  citeseer AttributeMask
Load full supervised task.
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\normalization.py:116: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
nclass: 6	nfeat:256
=== Number of Total Layers is 3 ===
Test set results: loss= 0.7728 auc= 0.9260 accuracy= 0.7630
accuracy=0.76300
0.422216	0.000000	0.772805	0.849890	0.000000	0.763000
Self-Supervised Type: AttributeMask~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='citeseer', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=5, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='AttributeMask~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.5446348061316502
saving model: AttributeMask~PCA on dataset:  citeseer
running on:  citeseer NodeProperty
Load full supervised task.
C:\Users\Yasmin\PycharmProjects\Graph_Anomaly\SSL_pretask\src\normalization.py:116: RuntimeWarning: divide by zero encountered in power
  r_inv = np.power(rowsum, -1).flatten()
nclass: 6	nfeat:256
=== Number of Total Layers is 3 ===
Test set results: loss= 0.7748 auc= 0.9252 accuracy= 0.7730
accuracy=0.77300
0.411361	0.000000	0.774834	0.857616	0.000000	0.773000
Self-Supervised Type: NodeProperty~PCA
Namespace(aggrmethod='add', alpha=1, cuda=False, datapath='C:\\Users\\Yasmin\\PycharmProjects\\Graph_Anomaly\\SelfTask-GNN\\data', dataset='citeseer', debug=False, dropout=0.5, early_stopping=0, epochs=100, fastmode=False, hidden=128, inputlayer='gcn', lambda_=0.1, lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=1, nhiddenlayer=1, no_cuda=True, no_tensorboard=True, normalization='AugNormAdj', outputlayer='gcn', param_searching=0, pca=1, sampling_percent=1.0, seed=42, ssl='NodeProperty~PCA', task_type='full', train_size=0, type='resgcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False, write_res=0)
len(idx_train)/len(adj.shape[0])=  0.5446348061316502
saving model: NodeProperty~PCA on dataset:  citeseer
task=NodeProperty~PCA, dataset=cora, test loss=EdgeMask, accuracy test=0.4457333981990814
task=NodeProperty~PCA, dataset=cora, test loss=PairwiseDistance, accuracy test=0.40762564539909363
task=NodeProperty~PCA, dataset=cora, test loss=PairwiseAttrSim, accuracy test=0.4636928141117096
task=NodeProperty~PCA, dataset=cora, test loss=Distance2Labeled, accuracy test=0.45873427391052246
task=NodeProperty~PCA, dataset=cora, test loss=AttributeMask, accuracy test=0.5168024301528931
task=NodeProperty~PCA, dataset=cora, test loss=NodeProperty, accuracy test=0.51413494348526
task=NodeProperty~PCA, dataset=citeseer, test loss=EdgeMask, accuracy test=0.7690582275390625
task=NodeProperty~PCA, dataset=citeseer, test loss=PairwiseDistance, accuracy test=0.7429031133651733
task=NodeProperty~PCA, dataset=citeseer, test loss=PairwiseAttrSim, accuracy test=0.7856116890907288
task=NodeProperty~PCA, dataset=citeseer, test loss=Distance2Labeled, accuracy test=0.7798412442207336
task=NodeProperty~PCA, dataset=citeseer, test loss=AttributeMask, accuracy test=0.7728049159049988
task=NodeProperty~PCA, dataset=citeseer, test loss=NodeProperty, accuracy test=0.7748338580131531

Process finished with exit code 0
 Epoch: 100